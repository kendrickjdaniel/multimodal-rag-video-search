# -*- coding: utf-8 -*-
"""Multimodal RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kLcVo55IfWK9bdhWVLiGaH9zGJv-aNnn

# Step 1 - Setup
"""

from google.colab import userdata
api_key = userdata.get('genai_course')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/GenAI/RAG/Multimodal RAG

"""# Step 2 - Get The Data"""

#Define the Video Path
video_path = "decision-making-course.mp4"

"""# Step 3 - Extract The Audio and Compress"""

# Install libararies
!pip install -q pydub
!apt-get install -q ffmpeg

#Import libraries
import os
import subprocess
from pydub import AudioSegment

#Define the audio path
audio_output_path = "audios/output.mp3"

#Ensure the output directory exists
output_dir = os.path.dirname(audio_output_path)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

#Ensure the outpit file has the correct extension
if not audio_output_path.endswith(".mp3"):
    audio_output_path += ".mp3"

#Construct the ffmpeg command to extract audio
command =[
    'ffmpeg',
    '-y', #overwrites if the audio exists
    '-i', video_path, #input file
    '-vn', # No video
    '-acodec', 'libmp3lame', #Audio codecs
    audio_output_path # Output file
]

#Execute the command to extract the audio
subprocess.run(command, check = True)

#Set the Bitrate
bitrate = "32k"

# Set path for compressed audio
compressed_audio_path = "audios/compressed.mp3"

#Construct the ffmpeg command to compress the audio
command = [
    'ffmpeg',
    '-y', #Overwrite if needed
    '-i', audio_output_path,
    '-ab', bitrate,
    compressed_audio_path
]
subprocess.run(command, check = True)

"""# Step 4 - Transcribe Audio Using OpenAI API"""

! pip install -q openai

#Library
from openai import OpenAI

#Connect the script to the API
client = OpenAI(api_key = api_key)

#Open the compressed file in binary mode
with open(compressed_audio_path, "rb") as file:
  #Use the Whisper model to transcribe
  transcript = client.audio.transcriptions.create(
      model = "whisper-1",
      file = file
  )

#Inspect the Transcript
transcript.text

#Define the path where transcription will be saved
transcript_path = "transcripts/transcript.txt"

#Save the transcribed text to a file
with open(transcript_path, "w") as file:
  file.write(transcript.text) #.text. is the attribute from transcription

# Alternative to the method directly ABOVE
#Save the transcribed text without using "with"
#file = open(transcript_path, "w")
#file.write(transcript.text)
#file.close()

"""# Step 5 - Extract Frames From The Video"""

#Load the library
!pip install -q moviepy

#Load The Video
from moviepy.editor import VideoFileClip

#Define output folder
output_folder = "frames"
if not os.path.exists(output_folder):
  os.makedirs(output_folder)

#load the video
video = VideoFileClip(video_path)

#Extract The Frames
frame_paths = []
interval = 10
for t in range(0, int(video.duration), interval):
  frame_path = os.path.join(output_folder, f"frame_{t:04d}.png")
  #Save The Frame At Specified Time
  video.save_frame(frame_path, t)
  frame_paths.append(frame_path)

"""# Step 6 - Embedding Audio"""

# Import libraries
from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer
import torch
import numpy as np

#Load the Model, Processer and Tokenizer
model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")

#Get the transcribed text
with open(transcript_path, "r") as file:
  transcript_text = file.read()

#Tokenize the entire text
tokenized_output = tokenizer(transcript_text,
                   return_tensors = "pt",
                   padding = True)
tokens = tokenized_output['input_ids'][0]
print(f"The number of tokens is {len(tokens)}")

#The CLIP Model requires 77 tokens per chunk
max_tokens = 77
transcription_chunks =[]

for i in range(0, len(tokens), max_tokens):
  chunk = tokens[i:i+max_tokens]
  transcription_chunks.append(chunk)

print(f" The number of chunks is {len(transcription_chunks)}")

#Text Embeddings -> embed the tokens in each chunk
text_embeddings = []
for chunk in transcription_chunks:
  # Ensure the chunk is in the correct shape
  inputs = {"input_ids": chunk.unsqueeze(0)}
  # Get the text embedding
  with torch.no_grad():
    text_embedding = model.get_text_features(**inputs)
    text_embeddings.append(text_embedding.cpu().numpy().flatten())

#Convert the list of embedding to a numpy array
text_embedding_np = np.array(text_embeddings)
#Print the shape
print(f"The shape of the text embedding is {text_embedding_np.shape}")

"""# Step 7 - Embedding The Images"""

from PIL import Image

#Embed The Images
frames_folder = "frames"
image_embeddings = []
image_paths = []

for frame_file in os.listdir(frames_folder):
  if frame_file.endswith('.png'):
    frame_path = os.path.join(frames_folder, frame_file)
    image_paths.append(frame_path)

    #load and prepocess the image
    image = Image.open(frame_path)
    inputs = processor(images = image, return_tensors = "pt")

    #Generate image embedding
    with torch.no_grad():
      image_embedding = model.get_image_features(**inputs)
      image_embeddings.append(image_embedding.cpu().numpy().flatten())

#Convert the list of embeddings to a numpy array
image_embeddings_np = np.array(image_embeddings)

#Print the shape
print(f"The shape of the image embeddings is {image_embeddings_np.shape}")

"""# Step 8 - Contrastive Learning"""

from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import random

#Calculate the cosine similarity matrtix
similarities = cosine_similarity(text_embedding_np,
                                 image_embeddings_np)
similarities

#Check the shape of the similarities
print(f"The shape of the similarities is {similarities.shape}")

#Retreive the top-k similar images for each text chunk
top_k = 5
for i, text_chunk in enumerate(similarities):
  similar_indices = text_chunk.argsort()[-top_k:][::-1]
  print(f"Top {top_k} images for each chunk {i}: {similar_indices} ")

# Set a randome seed for reproduciblity
random.seed(1502)

#Select 5 random text chunk indices
random_text_indices = random.sample(range(len(transcription_chunks)), 5)
print(f"Random Text Chunk Indices: {random_text_indices}")

# Find the 3 most similar images for each text
text_to_images_similarities = []
for idx in random_text_indices:
  similar_images = similarities[idx].argsort()[-3:][::-1]
  text_to_images_similarities.append(similar_images)

"""Gemini developed the code in the cell below. This code could have been a bit more simple. Keep this is mind as you review"""

# Display the selected text chunks and their most similar images
num_random_chunks = len(random_text_indices)

for i in range(num_random_chunks):
    text_idx = random_text_indices[i]
    similar_image_indices = text_to_images_similarities[i]

    # Get the original text chunk for context
    original_text_chunk = tokenizer.decode(transcription_chunks[text_idx])

    print(f"\n--- Text Chunk {text_idx} ---")
    print(f"Text (first 100 chars): {original_text_chunk[:100]}...")

    plt.figure(figsize=(15, 5))
    plt.suptitle(f"Text Chunk {text_idx} - Most Similar Images (Snippet: '{original_text_chunk[:70]}...')", fontsize=16)

    for j, img_idx in enumerate(similar_image_indices):
        # Ensure img_idx is within the bounds of image_paths
        if img_idx < len(image_paths):
            img_path = image_paths[img_idx]
            image = Image.open(img_path)

            plt.subplot(1, len(similar_image_indices), j + 1)
            plt.imshow(image)
            plt.title(f"Image {img_idx}")
            plt.axis('off')
        else:
            print(f"Warning: Image index {img_idx} out of bounds for image_paths. Skipping.")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap
    plt.show()

"""#Step 9 - Retrieval System"""

#Let's define a query
query = "Which cognitive biases are discussed?"

#Tokenize the query
query_tokens = tokenizer(query,
                         return_tensors = "pt",
                         padding = True)['input_ids']

#Generate the query embedding in the join embedding space (Use the CLIP model)
with torch.no_grad():
  query_embedding = model.get_text_features(
      input_ids = query_tokens
  ).cpu().numpy().flatten()
  print(f"The shape of the query embedding is {query_embedding.shape}")

from sklearn.metrics.pairwise import cosine_similarity
#Compute the cosine similarity between the query and the transcripts
query_text_similarities = cosine_similarity([query_embedding],text_embedding_np)[0]
query_text_similarities

#define how many chunks we want
top_k_texts = 10

#Retrieve the indices of the top-k most similar text_chunks
top_k_indices = query_text_similarities.argsort()[-top_k_texts:][::-1]
top_k_indices

"""Gemini developed the code in the cell below. This code could have been a bit more simple. Keep this is mind as you review"""

print(f"\nTop {top_k_texts} most similar text chunks to the query:\n")
for i, idx in enumerate(top_k_indices):
    original_text_chunk = tokenizer.decode(transcription_chunks[idx])
    print(f"--- Rank {i+1} (Chunk {idx}) ---")
    print(original_text_chunk)
    print("\n")

"""# Step 10 - Generation System"""

import base64

#Combine the retrieved text chunks
retrieved_text_list = []
for idx in top_k_indices:
  retrieved_text_list.append(tokenizer.decode(transcription_chunks[idx]))
retrieved_text = "".join(retrieved_text_list)
retrieved_text

#Convert the images and append them
base64_images = []
for idx in top_k_indices:
  image_path = image_paths[idx]
  with open(image_path, "rb") as image_file:
    base64_image = base64.b64encode(image_file.read()).decode('utf-8')
    base64_images.append(base64_image)
base64_images

#Define the mode and system prompt
MODEL = "gpt-4o-mini"
system_prompt = """
You are an expert teacher that summarizes visual and transcribed content
"""

#Prepare the user message content
user_message_content = [
    "There are the frames from the video",
    *map(lambda x: {"type": "image_url",
                    "image_url": {"url": f'data:image/jpg;base64,{x}'}
                    },
         base64_images),
    {"type": "text",
     "text": retrieved_text}
]

#Call the OpenAI API to generate a summary
response = client.chat.completions.create(
    model = MODEL,
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message_content},
    ],
    temperature = 0.3,
)

#Generate the response
generated_response = response.choices[0].message.content
print(generated_response)